{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define filepaths. You will need to change the filepaths to reflect your own directory structure. Just copy and paste into the \"\".\n",
    "For reading in the HRV files, the below list comprehension looks for any files starting with 'p' and ending with '.csv'.\n",
    "This reflects the file naming convention used for the HRV files. If you rename files or the naming convention is changed, then you will have to adapt this to reflect the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = r\"P:\\Spironolactone\\main_qualtrics\"\n",
    "main_filename = \"main_dat21.csv\"\n",
    "hrv_dir = r\"P:\\Spironolactone\\Firstbeat\"\n",
    "hrv_files = [file for file in os.listdir(hrv_dir) if file.lower().startswith('p') and file.endswith('.csv')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make an output directory (or issue a warning if it already exists). I put mine into the directory where the rest of the HRV files sit and named it \"processed_hrv_files\" but you are of course free to choose your own name and location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-3de6f22e24f9>:6: UserWarning: Directory already exists. Files may be overwritten. Manual check advised.\n",
      "  warnings.warn(\"Directory already exists. Files may be overwritten. Manual check advised.\")\n"
     ]
    }
   ],
   "source": [
    "output_dir = os.path.join(hrv_dir,\"processed_hrv_files\")\n",
    "try:\n",
    "    os.makedirs(output_dir)\n",
    "except OSError:\n",
    "    # if directory already exists\n",
    "    warnings.warn(\"Directory already exists. Files may be overwritten. Manual check advised.\")\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in the qualtrics file from the study day session and retaining/renaming the columns we need below.\n",
    "I would NOT advise changing the 'new_names' unless you are happy to change them in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list =  [\"Status\",\"DQ-1\",\"Firstbeat_on_time\",\"baseline start\",\"baseline end\",\"Q645\",\"Q646\",\"FILM-START\",\"Q648\",\"Q649\"]\n",
    "new_names = [\"response_type\",\"Participant_number\",\"Firstbeat_start\",\"RT1_start\",\"RT1_end\",\"RT2_start\",\"RT2_end\",\"Film_start\",\"RT3_start\",\"RT3_end\"]\n",
    "qualtrics_df = pd.read_csv(os.path.join(main_dir,main_filename),usecols =col_list,skiprows= [1,2])\n",
    "qualtrics_df.columns = new_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilities.\n",
    "All the functions used in the main body of the script below are defined here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_invalid_records(in_df, id_col,\n",
    "    exclude_pnums = None, max_val = 100):\n",
    "    \"\"\"\n",
    "    remove invalid records - participant id is nan or\n",
    "    >100 (usually indicates test record)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_df:  pd DataFrame\n",
    "        dataframe to operate on\n",
    "    id_col: str\n",
    "        name of column containing\n",
    "        participant ids\n",
    "    max_val: int\n",
    "        max value for participant ID to be\n",
    "        valid\n",
    "    exclude_pnums: list[int]\n",
    "        specify participants who should be \n",
    "        excluded from the analysis, if any\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        dataframe w/o the above records\n",
    "    \"\"\"\n",
    "    in_df = in_df[(in_df[id_col].notna())\n",
    "                    &(in_df[id_col]<max_val)]\n",
    "    if exclude_pnums:\n",
    "        in_df = in_df.drop(labels = in_df[\n",
    "                in_df[id_col].isin(exclude_pnums)].index,\n",
    "                axis = 0)\n",
    "    return in_df\n",
    "    \n",
    "def remove_duplicate_participants(in_df, id_col):\n",
    "    \"\"\"\n",
    "    If we have duplicate records\n",
    "    for a given participant, remove\n",
    "    that with the most NaNs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_df:  pd Dataframe\n",
    "        dataframe to operate on\n",
    "    id_col: str\n",
    "        name of column containing\n",
    "        participant ids\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        in_df w/o all nan duplicates\n",
    "    \"\"\"\n",
    "    duplicated = in_df.loc[in_df.duplicated(subset = id_col),id_col]\n",
    "    if duplicated is None:\n",
    "        return in_df\n",
    "    else:\n",
    "        drop_inds = []\n",
    "        for dup in duplicated:\n",
    "            nan_sum = in_df[in_df[id_col] == dup].isna().sum(axis = 1)\n",
    "            nan_max_ind = nan_sum[nan_sum == nan_sum.max()].index\n",
    "            drop_inds.append(nan_max_ind[0])\n",
    "        in_df = in_df.drop(labels = drop_inds, axis = 0)\n",
    "        return in_df       \n",
    "\n",
    "def flag_duplicate_participants(in_df, id_col):\n",
    "    \"\"\"\n",
    "    If we have duplicate records\n",
    "    for a given participant, flag\n",
    "    them.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_df:  pd Dataframe\n",
    "    id_col: str\n",
    "        name of column containing\n",
    "        participant ids\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        participant numbers for those\n",
    "        with duplicate records\n",
    "    \"\"\"\n",
    "    duplicated = in_df.loc[in_df.duplicated(subset = id_col),id_col]\n",
    "    print(f\"The following participants have duplicate records:\\n{duplicated.values}\")\n",
    "    return duplicated         \n",
    "\n",
    "def convert_time_cols(in_df):\n",
    "    \"\"\"\n",
    "    convert time cols to datetime format\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    in_df:  pd Dataframe\n",
    "        dataframe to operate on\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dataframe with time cols converted\n",
    "    to datetime format.\n",
    "    \"\"\"\n",
    "    time_cols = [col for col in in_df.columns\n",
    "                if any(k in col for k in [\"start\",\"end\"])]\n",
    "    in_df.loc[:,time_cols] = in_df.loc[:,time_cols].apply(\n",
    "                                lambda x: pd.to_datetime(x,errors = \"coerce\"),\n",
    "                                axis = 1)\n",
    "    return in_df\n",
    "\n",
    "def add_end_time(in_df,start_time_col, amount):\n",
    "    \"\"\"\n",
    "    If we don't have a time for the interval end,\n",
    "    provide an end time in minutes from start time.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_df:  pd dataframe\n",
    "        dataframe to operate on\n",
    "    start_time_col: str\n",
    "        name of column with interval start time\n",
    "        eg. Film_start\n",
    "    amount: int\n",
    "        number of minutes to add to start time\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    in_df with end_time column added.\n",
    "    \"\"\"\n",
    "    new_col_name = \"_\".join([start_time_col.split(\"_\")[0],\"end\"])\n",
    "    in_df[new_col_name] = in_df[start_time_col] + datetime.timedelta(minutes = amount)\n",
    "    return in_df\n",
    "\n",
    "def make_rel_time_cols(in_df,time_col_start,time_col_end):\n",
    "    \"\"\"\n",
    "    calculate relative timings for time columns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_df:  pandas Dataframe\n",
    "        dataframe to operate on\n",
    "    time_col_start: str\n",
    "        name of start time column\n",
    "    time_col_end:   str\n",
    "        name of end time column\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    in_df with rel time columns added\n",
    "    \"\"\"\n",
    "    new_col_name = \"_\".join([time_col_end,\"interval\"])\n",
    "    in_df[new_col_name] = in_df[time_col_end]-in_df[time_col_start]\n",
    "    return in_df\n",
    "\n",
    "def convert_to_secs(in_df):\n",
    "    \"\"\"\n",
    "    convert time delta to seconds.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_df:  pd Dataframe\n",
    "        dataframe to operate on\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    interval cols converted to secs\n",
    "    from Firstbeat start time.\n",
    "    \"\"\"\n",
    "    interval_cols = in_df.filter(like = \"interval\",axis = 1).columns\n",
    "    in_df.loc[:,interval_cols] = in_df.loc[:,interval_cols].applymap(\n",
    "                                            lambda x: x.total_seconds())\n",
    "    return in_df\n",
    "\n",
    "def select_hrv_record(pnum,hrv_files):\n",
    "    \"\"\"\n",
    "    select hrv file for participant\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pnum:   int or float\n",
    "        participant number whos record\n",
    "        needs to be retrieved\n",
    "    hrv_files:  list[str]\n",
    "        list of hrv files\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        name of hrv file for participant pnum (str)\n",
    "    \"\"\"\n",
    "    recs = [file for file in hrv_files if int(file[1:4])==pnum]\n",
    "    if len(recs)>1:\n",
    "        print(f\"Found more than one file for participant {pnum}.\\nManual check advised.\")\n",
    "    return recs.pop()\n",
    "\n",
    "def get_time_stamp(in_df,id_col,interval_col,pnum):\n",
    "    \"\"\"\n",
    "    Get time stamp for processing HRV files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_df:  pd Dataframe\n",
    "        input dataframe\n",
    "    interval_col:   str\n",
    "        name of column with desired interval\n",
    "        eg \"Film_start_interval\"\n",
    "    pnum:   int\n",
    "        participant number\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    time stamp (for input to get_hrv_interval())\n",
    "    \"\"\"\n",
    "    time_stamp = in_df[in_df[id_col]==pnum].reset_index().at[0,interval_col]\n",
    "    return time_stamp\n",
    "\n",
    "def get_hrv_interval(hrv_df,interval_start:float,interval_end:float):\n",
    "    \"\"\"\n",
    "    Get intervals for HRV data\n",
    "    This function will find the closest value to the\n",
    "    specified start and end times.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hrv_df: pd DataFrame\n",
    "        HRV data for a given participant\n",
    "    interval_start: float\n",
    "        start time as duration in seconds from\n",
    "        start of Firstbeat (ie Film_start for participant 1)\n",
    "    interval_end:   float\n",
    "\n",
    "    \"\"\"\n",
    "    hrv_df[\"IBI_cumsum\"] = hrv_df.IB_intervals.cumsum()/1000\n",
    "    start_vals = (hrv_df.IBI_cumsum-interval_start).sub(0).abs().idxmin()\n",
    "    end_vals = (hrv_df.IBI_cumsum-interval_end).sub(0).abs().idxmin()\n",
    "    interval_df = hrv_df.iloc[start_vals:end_vals]\n",
    "    return interval_df[\"IB_intervals\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing.\n",
    "Remove_invalid_records: Gets rid of records with invalid or missing participant numbers. Invalid is defined as >100. This usually means that we were testing the qualtrics survey (we often used participant numbers such as 999).\n",
    "flag_duplicate_participants: Print participant numbers for participants who have more than one record in the main qualtrics survey.\n",
    "remove_duplicate_participants: remove duplicate records based on which of the two has more NaNs. You can comment out this step if you would not like to remove them, but this will leave you with two records and will most likely cause errors below.\n",
    "If you would like to remove one record manually, I would advise retrieving the duplicate records like so: qualtrics_df[qualtrics_df.Participant_number == pnum] where pnum is the participant number that was flagged as having duplicates. This will show you all records and you could then remove one of them (eg using qualtrics_df.drop(labels = pnum_index, axis = 0), where pnum_index is the row index of the record your want to drop).\n",
    "convert_time_cols: convert time related columns to datetime format\n",
    "add_end_time: Just used for the film. This is because there was no automatic time capture to indicate the end of the film. Basically, I just added 15 minutes to the film start time to get this. You can adjust this by changing the second positional argument to whatever number of minutes you think is best (it has to be in minutes).\n",
    "make_rel_time_cols: Calculate the interval from the start of the firstbeat recording to the different interval start/end times (Film, RTs)\n",
    "convert_to_secs: Convert the above interval columns to seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following participants have duplicate records:\n",
      "[ 4. 44.]\n"
     ]
    }
   ],
   "source": [
    "qualtrics_df = remove_invalid_records(qualtrics_df,\"Participant_number\",exclude_pnums = [1])\n",
    "duplicates = flag_duplicate_participants(qualtrics_df,\"Participant_number\")\n",
    "qualtrics_df = remove_duplicate_participants(qualtrics_df,\"Participant_number\")\n",
    "qualtrics_df = convert_time_cols(qualtrics_df)\n",
    "qualtrics_df = add_end_time(qualtrics_df,\"Film_start\",15)\n",
    "rt_time_cols = [f for f in qualtrics_df.columns if any(k in f for k in [\"start\",\"end\"])]\n",
    "for rt_time in rt_time_cols[1:]:\n",
    "    qualtrics_df = make_rel_time_cols(qualtrics_df,rt_time_cols[0],rt_time)\n",
    "qualtrics_df = convert_to_secs(qualtrics_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we process the actual HRV files. I've commented the main steps below, but here's the gist:\n",
    "For each participant, load the hrv file, get the start/end intervals for each of the different sections (Film, RTs)\n",
    "get_hrv_intervals gets the relevant sections from a participant's HRV file by calculating the cumulative sum of the interbeat intervals, subtracting interval start/end times from this and identifying the index of the row where the result of this subtraction is closes to 0.\n",
    "The resulting files, containing HRV data for each individual and participant, are then saved to output_dir.\n",
    "If there is something wrong with the HRV file or the interval data (for instance, because a tag was missed or the Firstbeat cut out, resulting in missing data), this will result in a warning and the participant number and interval named being appended to the missing_pnums list. You can then access this following processing of the hrv files to conduct manual checks on those participants to see what happened. Mostly it seems to be the firstbeat having cut out, resulting in no data for some intervals. Note that NO csv file will be saved for affected participants/intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Participant 7.0 has no valid data for Film interval.\n",
      "Manual check advised. Skipping.\n",
      "Participant 7.0 has no valid data for RT1 interval.\n",
      "Manual check advised. Skipping.\n",
      "Participant 7.0 has no valid data for RT2 interval.\n",
      "Manual check advised. Skipping.\n",
      "Participant 7.0 has no valid data for RT3 interval.\n",
      "Manual check advised. Skipping.\n",
      "Participant 10.0 has no valid data for Film interval.\n",
      "Manual check advised. Skipping.\n",
      "Participant 10.0 has no valid data for RT2 interval.\n",
      "Manual check advised. Skipping.\n",
      "Participant 10.0 has no valid data for RT3 interval.\n",
      "Manual check advised. Skipping.\n",
      "No HRV file found for participant 11.0.\n",
      "Found more than one file for participant 25.0.\n",
      "Manual check advised.\n",
      "Participant 25.0 has no valid data for Film interval.\n",
      "Manual check advised. Skipping.\n",
      "Participant 25.0 has no valid data for RT2 interval.\n",
      "Manual check advised. Skipping.\n",
      "Participant 25.0 has no valid data for RT3 interval.\n",
      "Manual check advised. Skipping.\n",
      "Participant 13.0 has no valid data for RT2 interval.\n",
      "Manual check advised. Skipping.\n",
      "Found more than one file for participant 34.0.\n",
      "Manual check advised.\n",
      "Start or end of interval for participant 39.0 is nan. Indexing not possible. Skipping.\n"
     ]
    }
   ],
   "source": [
    "# Zip up start/end interval column names for easier access in for loop below:\n",
    "start_intervals = qualtrics_df.filter(like = \"start_interval\", axis = 1).columns.sort_values()\n",
    "end_intervals = qualtrics_df.filter(like = \"end_interval\", axis = 1).columns.sort_values()\n",
    "intervals = list(zip(start_intervals, end_intervals))\n",
    "\n",
    "# Track participants whose HRV data for any of the intervals is missing\n",
    "missing_pnums = []\n",
    "for pnum in qualtrics_df.Participant_number:\n",
    "    # check if file exists\n",
    "    try:\n",
    "        my_rec = select_hrv_record(pnum,hrv_files)\n",
    "    except IndexError:\n",
    "        print(f\"No HRV file found for participant {pnum}.\")\n",
    "        continue\n",
    "    # read in HRV file for participant pnum\n",
    "    hrv_df = pd.read_csv(\n",
    "                        os.path.join(hrv_dir,my_rec),\n",
    "                        header = 0, names = [\"IB_intervals\"],\n",
    "                        skiprows = np.arange(0,4)\n",
    "                        )\n",
    "    # select the part of the HRV file that corresponds to given interval\n",
    "    # do this for all intervals (Film, RT1, RT2, RT3)\n",
    "    for start_interval, end_interval in intervals:\n",
    "        start_time = get_time_stamp(qualtrics_df,\"Participant_number\", start_interval, pnum)\n",
    "        end_time = get_time_stamp(qualtrics_df, \"Participant_number\",end_interval, pnum)\n",
    "        try:\n",
    "            interval_df = get_hrv_interval(hrv_df,start_time,end_time)\n",
    "        except TypeError:\n",
    "            print(f\"Start or end of interval for participant {pnum} is {start_time}. Indexing not possible. Skipping.\")\n",
    "            continue\n",
    "        # if the resulting dataframe is empty, flag this and hold on to pnum/interval\n",
    "        if interval_df.empty:\n",
    "            interval_name = start_interval.split(\"_\")[0]\n",
    "            print(f\"Participant {pnum} has no valid data for {interval_name} interval.\\nManual check advised. Skipping.\")\n",
    "            missing_pnums.append([pnum,interval_name])\n",
    "            continue\n",
    "        # save to file\n",
    "        interval_df.to_csv(os.path.join(output_dir, \"_\".join([start_interval.split(\"_\")[0],str(int(pnum)),\"hrv.csv\"])),index = False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "48186e61764c8c514947f0ef500accf59797b98e64cdc910e21ec2975c1f1025"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
