{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = r\"P:\\Spironolactone\\E4\"\n",
    "main_qualtrics_dir = r\"P:\\Spironolactone\\main_qualtrics\"\n",
    "participant_folders = os.listdir(input_dir)\n",
    "participant_folders = [f for f in participant_folders if re.search(\"^p[0][0-9][0-9]\",f.lower())] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_participant_num(folder_name):\n",
    "    \"\"\"\n",
    "    Get participant number from folder\n",
    "    name.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    folder_name:    str\n",
    "        Name of participant e4 data folder\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Participant number as an integer value.\n",
    "\n",
    "    \"\"\"\n",
    "    pnum = re.findall(\"p[0][0-9][0-9]\",folder_name.lower())\n",
    "    return int(pnum.pop()[1:])\n",
    "\n",
    "def flag_duplicates(folder_name):\n",
    "    \"\"\"\n",
    "    Flag duplicate E4 folders.\n",
    "    Happens when E4 recording was\n",
    "    interrupted for some reason.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    folder_name:    str\n",
    "    Name of participant e4 data folder\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of duplicate participant numbers.\n",
    "    \"\"\"\n",
    "    parts = [num[:4] for num in folder_name]\n",
    "    dups = [int(num[1:]) for num, count in Counter(parts).items() if count>1]\n",
    "    return dups\n",
    "\n",
    "def remove_multindex(in_df, axis, level):\n",
    "    \"\"\"\n",
    "    Remove multi index of specified level\n",
    "    along specified axis.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_df:  pd DataFrame\n",
    "        df to operate on\n",
    "    axis:   int\n",
    "        rows = 0\n",
    "        cols = 1\n",
    "    level:  int\n",
    "        level of multindex to remove.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dataframe with specified level of multiindex\n",
    "    removed\n",
    "    \"\"\"\n",
    "    if axis == 0:\n",
    "        out_df = in_df.reset_index(level = level, drop = True)\n",
    "    else:\n",
    "        out_df = in_df.T.reset_index(level = level, drop = True).T\n",
    "    return out_df\n",
    "    \n",
    "def get_rowdiff(values):\n",
    "    \"\"\"\n",
    "    calculate difference\n",
    "    between rows of series\n",
    "\n",
    "    Paramters\n",
    "    ---------\n",
    "    values\n",
    "\n",
    "    Returns\n",
    "        differences between successive\n",
    "        rows\n",
    "    \"\"\"\n",
    "    if len(values)<=1:\n",
    "        return values\n",
    "    else:\n",
    "        new_vals = []\n",
    "        for i,val in enumerate(values):\n",
    "            if i==0:\n",
    "                new_val = val\n",
    "            else:\n",
    "                new_val = val-values[i-1]\n",
    "            new_vals.append(new_val)\n",
    "    return new_vals\n",
    "\n",
    "def find_min_delta(tag_deltas, axis = 0):\n",
    "    \"\"\"\n",
    "    find minimum difference\n",
    "    between successive tags.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tag_deltas: pd DataFrame\n",
    "        dataframe of row-to-row differences\n",
    "    axis:   int\n",
    "        0 for min row  w/in col\n",
    "        1 for min col w/in row\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        min value for each column (participant)\n",
    "    \"\"\"\n",
    "    min_diffs = tag_deltas.apply(lambda x: min(x),axis = axis)\n",
    "    return min_diffs\n",
    "\n",
    "def return_likely_doubles(min_deltas, time_delta_df, threshold):\n",
    "    \"\"\"\n",
    "    find likely double tags\n",
    "    based on min delta\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    min_deltas: pd Series\n",
    "        minimum difference between rows\n",
    "        for each participant\n",
    "    time_delta_df:  pd DataFrame\n",
    "        dataframe representing differences\n",
    "        between successive rows\n",
    "    threshold:  float\n",
    "        min_deltas will be multiplied by\n",
    "        threshold to define an acceptable\n",
    "        window for identifying double tags\n",
    "        eg: look for row differences less than\n",
    "        1.5*min_deltas, where 1.5 would represent\n",
    "        thresh\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        time_delta_df masked with nan where\n",
    "        threshold exceeded. Non-nan values\n",
    "        represent possible double tags.\n",
    "    \"\"\"\n",
    "    double_tag_df = time_delta_df[\n",
    "                                time_delta_df<=min_deltas*threshold\n",
    "                                ].dropna(how = \"all\",axis = 0)\n",
    "    return double_tag_df\n",
    "\n",
    "def get_num_double_tags(double_tag_df):\n",
    "    \"\"\"\n",
    "    Get number of double tags\n",
    "    detected in dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    double_tag_df:  pd DataFrame\n",
    "        dataframe with value for likely\n",
    "        double tags, NaN elswhere\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    value counts of double tags identfied\n",
    "    \"\"\"\n",
    "    double_counts = double_tag_df.apply(\n",
    "                                        lambda x:\n",
    "                                        x.notna().sum()\n",
    "                                        ).value_counts()\n",
    "    return double_counts\n",
    "\n",
    "def get_best_thresh(tag_val_counts):\n",
    "    \"\"\"\n",
    "    get threshold that maximises\n",
    "    number of double tags detected\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tag_val_counts: list of tuples\n",
    "        of type [(two_counts, threshold)]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        Threshold at which max number of \n",
    "        two double tags were detected.\n",
    "\n",
    "    \"\"\"\n",
    "    max_num = max([val[0] for val in tag_val_counts])\n",
    "    max_threshold = [thresh for (num,thresh)\n",
    "                    in tag_val_counts if num == max_num]\n",
    "    if len(max_threshold)>1:\n",
    "        print(f\"More than 1 max val detected. Manual check advised.\")\n",
    "    return max_threshold\n",
    "\n",
    "def detect_missing_doubles(double_tags_df):\n",
    "    \"\"\"\n",
    "    Flag participants with (possible)\n",
    "    missing double_tags.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    num_tags = double_tags_df.apply(\n",
    "                                    lambda x: len(x.dropna())\n",
    "                                    )\n",
    "    flags_df = num_tags[num_tags != 2]\n",
    "    flags_df = flags_df.reset_index()\n",
    "    flags_df.columns = [\"pnum\",\"num_double_tags\"]\n",
    "    return flags_df\n",
    "\n",
    "\n",
    "def check_pnums(*args):\n",
    "    \"\"\"\n",
    "    Get a single list of all\n",
    "    pnums worth double checking\n",
    "    for any reason (duplicates,\n",
    "    <min tags, qualtrics notes detected)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    args:   list\n",
    "        lists of participant numbers\n",
    "        for which problems have been\n",
    "        detected.\n",
    "        Eg: duplicates, below_min\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A single list of participant numbers\n",
    "    to check.\n",
    "    \"\"\"\n",
    "    all_pnums = []\n",
    "    for arg in args:\n",
    "        all_pnums.extend(arg)\n",
    "    return set(all_pnums)\n",
    "\n",
    "def find_e4_notes(study_session_df,notes_col, pnum_col, keywords):\n",
    "    \"\"\"\n",
    "    Get participant numbers for whom E4 related\n",
    "    issues were flagged in the data acquisition \n",
    "    session.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    study_session_df:   pd DataFrame\n",
    "        data frame containing main study session\n",
    "        data. Must have participant number and session\n",
    "        notes.\n",
    "    notes_col, pnum_col:    str\n",
    "        names of session notes/participant number columns\n",
    "    keywords:   list[str]\n",
    "        a list of strings representing keywords to look for\n",
    "        in the session notes.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    participant numbers for whom E4 related events were recorded\n",
    "    in the session notes.\n",
    "    \"\"\"\n",
    "    reg_substr = \"|\".join(keywords)\n",
    "    flagged_participants = study_session_df.loc[study_session_df[\n",
    "                            notes_col].str.lower().str.contains(reg_substr),\n",
    "                            pnum_col].astype(int).values\n",
    "    return flagged_participants\n",
    "\n",
    "def check_double_tags(double_tag_df, num_tags):\n",
    "    \"\"\"\n",
    "    This function lets you inspect participants\n",
    "    with unusual double tag numbers (below or above\n",
    "    2).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    double_tag_df:  pd DataFrame\n",
    "        dataframe of likely double tags\n",
    "    num_tags:   int\n",
    "                the number of tags to look for\n",
    "                eg 1, 3, 4...\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A dataframe showing only cols for participants\n",
    "    with the specified number of double tags.\n",
    "    OR\n",
    "    if no participants found for the specified number\n",
    "    of tags, this function will return None.\n",
    "    \"\"\"\n",
    "\n",
    "    double_view_df = double_tag_df.loc[\n",
    "                                        :,double_tag_df.notna().sum()==num_tags\n",
    "                                        ]\n",
    "    if double_view_df.shape[1]<=1:\n",
    "        print(\"No participants found for this number of tags.\")\n",
    "    else:\n",
    "        return double_view_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in tag files for all participants and assemble those w/o duplicates into a single dataframe. The steps are as follows:\n",
    "(i) Get folder for participant, extract participant number from folder name.\n",
    "    If participant pnum has more than one folder, skip this record and print a message to the console.\n",
    "    This will get printed twice for each participant.\n",
    "(ii) Retrieve tags file for participant pnum.\n",
    "    If no tags file found, print a message and move on.\n",
    "    If the number of tags is less than the minimum number of tags expected (14),\n",
    "    skip this participant and continue.\n",
    "(iii) Assemble tags into a single dataframe, with columns = participants and rows = tags.\n",
    "Note that participants with issues (more than one folder, no tags file) will be stored in duplicates, below_min and missing_tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one tag file exists for participant 1. Skipping.\n",
      "More than one tag file exists for participant 1. Skipping.\n",
      "More than one tag file exists for participant 10. Skipping.\n",
      "More than one tag file exists for participant 10. Skipping.\n",
      "Participant 12 recorded fewer than the minimum number of tags. Manual check advised.\n",
      "Participant 14 recorded fewer than the minimum number of tags. Manual check advised.\n",
      "More than one tag file exists for participant 20. Skipping.\n",
      "More than one tag file exists for participant 20. Skipping.\n",
      "More than one tag file exists for participant 27. Skipping.\n",
      "More than one tag file exists for participant 27. Skipping.\n"
     ]
    }
   ],
   "source": [
    "missing_tags = []\n",
    "pnums = []\n",
    "tags_dat = []\n",
    "below_min = []\n",
    "duplicates = flag_duplicates(participant_folders)\n",
    "\n",
    "for folder in participant_folders:\n",
    "    pnum = get_participant_num(folder)\n",
    "    if pnum in duplicates:\n",
    "        print(f\"More than one tag file exists for participant {pnum}. Skipping.\")\n",
    "        continue\n",
    "    try:\n",
    "        tags_df = pd.read_csv(os.path.join(input_dir,folder,\"tags.csv\"),header = 0,names = [pnum])\n",
    "    except FileNotFoundError:\n",
    "        print(f\"No tags file found for participant {pnum}.Manual check advised.\")\n",
    "        missing_tags.append(pnum)\n",
    "        continue\n",
    "    if tags_df.shape[0]<14:\n",
    "        print(f\"Participant {pnum} recorded fewer than the minimum number of tags. Manual check advised.\")\n",
    "        below_min.append(pnum)\n",
    "        continue\n",
    "    pnums.append(pnum)\n",
    "    tags_dat.append(tags_df)\n",
    "\n",
    "tags_df = pd.concat(tags_dat,axis = 1,keys =[pnum for pnum in pnums])\n",
    "tags_df = remove_multindex(tags_df,1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for likely double tags based on time between successive tags. There is no \"hard and fast\" criterion, but given the fact that (a) they're only supposed to double-tag twice and (b) double tags are supposed to happen in quick succession (with no other button press supposed to be quicker), the approach used here is based on the following steps:\n",
    "(i) Look for the smallest row difference for each participant (where participants = columns, rows = tags).\n",
    "(ii) Look for row differences within a certain above this minimum difference (threshold is chosen based on the value that maximises the number of double tags detected).\n",
    "(iii) Count the number of double tags detected.\n",
    "(iv) Flag those participants for whom (for a given threshold) either too few or too many double tags were detected.\n",
    "These participants can then be checked manually, or we can check whether the words 'tag' or 'E4' are mentioned in their session notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will calculate the difference between successive rows (ie tags)\n",
    "tags_diff_df = tags_df.apply(lambda x: get_rowdiff(x))\n",
    "# this finds the smallest difference in the df generated above\n",
    "min_deltas = find_min_delta(tags_diff_df)\n",
    "# this is a range of thresholds to try out to find the best tag difference\n",
    "# to detect double tags. The way it works is that min_deltas will be multiplied\n",
    "# by thresh and we will look for tag time differences below (or equal to) the resulting threshold.\n",
    "thresholds = np.arange(1.5,5,0.5)\n",
    "all_tag_vals = []\n",
    "# try out the different thresholds\n",
    "for thresh in thresholds:\n",
    "    double_df = return_likely_doubles(min_deltas, tags_diff_df, thresh)\n",
    "    tag_vals = get_num_double_tags(double_df)\n",
    "    num_twos = tag_vals[2]\n",
    "    all_tag_vals.append((num_twos,thresh))\n",
    "# get the threshold for which the largest number of double tags was recorded\n",
    "max_thresh = get_best_thresh(all_tag_vals)\n",
    "# return a dataframe of likely double tags where detected, NaN elswhere\n",
    "double_df = return_likely_doubles(min_deltas, tags_diff_df,max_thresh)\n",
    "# get an overview of pnums with double tags less or greater than 2 and the tags detected.\n",
    "detect_missing_doubles_df = detect_missing_doubles(double_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now add an events column to the \"likely double tag\" dataframe. This represents the events that should have been tagged, up to the first double tag that indicates the start of the music section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The following participants had double tags detected around the start of the music section:\n",
      "\n",
      "       2     3     4     5     6    8    9  11  13    15  ...    28    29  30  \\\n",
      "10  1.96  0.36   NaN  0.27  1.92  NaN  NaN NaN NaN  5.87  ...  5.91  1.57 NaN   \n",
      "11   NaN   NaN  0.48   NaN   NaN  5.5  6.4 NaN NaN   NaN  ...   NaN   NaN NaN   \n",
      "\n",
      "      31  32    33  34    35    36            Events  \n",
      "10  7.11 NaN  5.94 NaN  5.88  6.12  DT1_music_starts  \n",
      "11   NaN NaN   NaN NaN   NaN   NaN  DT2_music_starts  \n",
      "\n",
      "[2 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "double_df[\"Events\"] = pd.Series([\n",
    "                                \"Firstbeat\",\"RT1_start\",\"RT1_end\",\"Drug\",\n",
    "                                \"RT2_start\",\"RT2_end\",\"Film_start\",\"Film_end\",\n",
    "                                \"RT3_start\",\"RT3_end\",\"DT1_music_starts\",\n",
    "                                \"DT2_music_starts\"\n",
    "                                ])\n",
    "\n",
    "single_tags = check_double_tags(double_df,1)\n",
    "print(\"\\nThe following participants had double tags detected around the start of the music section:\\n\")\n",
    "print(double_df[double_df.Events.isin([\"DT1_music_starts\",\"DT2_music_starts\"])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for notes in the main qualtrics survey containing info regarding E4 tags. Here, we look for the words \"tag\" or \"e4\" to flag participants for whom problems with the E4 watch were reported. You can include more keywords by modifying the keywords variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualtrics_df = pd.read_csv(os.path.join(main_qualtrics_dir,\"main_dat.csv\"),skiprows = [1,2],usecols = [\"DQ-1\", \"NOTES\"])\n",
    "qualtrics_df.columns = [\"pnum\",\"session_notes\"]\n",
    "qualtrics_df = qualtrics_df.drop(labels = qualtrics_df[qualtrics_df.session_notes.isna()].index, axis = 0)\n",
    "keywords = [\"tag\",\"e4\"]\n",
    "flagged_participants = find_e4_notes(qualtrics_df,\"session_notes\",\"pnum\",keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 4 categories of participants for whom we recorded issues. We will now assemble these participant numbes into a single list using .extend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following participants are worth checking manually:\n",
      "{1, 3, 4, 9, 10, 12, 14, 16, 17, 18, 19, 20, 27, 29}\n"
     ]
    }
   ],
   "source": [
    "manual_check_pnums = check_pnums(flagged_participants,missing_tags, below_min,duplicates)\n",
    "print(f\"The following participants are worth checking manually:\\n{manual_check_pnums}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "48186e61764c8c514947f0ef500accf59797b98e64cdc910e21ec2975c1f1025"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
